services:
  frontend:
    build:
      context: ./front
      dockerfile: Dockerfile
    ports:
      - 80:80

  backend:
    build:
      context: ./back
      dockerfile: Dockerfile.dev
    ports:
      - 3000:3000
    env_file:
      - ./back.env
    volumes:
      - ./back:/app
    depends_on:
      - db

  db:
    image: postgres:15-alpine
    container_name: postgres_db
    environment:
      POSTGRES_USER: easydevs
      POSTGRES_PASSWORD: test123
      POSTGRES_DB: easydevs
    ports:
      - 5432:5432
    volumes:
      - ./back/migrations:/docker-entrypoint-initdb.d

  vllm:
    runtime: nvidia
    image: vllm/vllm-openai:latest
    restart: always
    shm_size: '15gb'
    env_file:
      - ./llm.env
    expose:
      - 8000
    volumes:
      - .vllm-cache:/workspace/.cache
    entrypoint: python3
    command: -m vllm.entrypoints.openai.api_server --port=8000 --host=0.0.0.0 --seed=1234 --trust-remote-code --download-dir=/workspace/.cache/huggingface/hub --model Qwen/Qwen2.5-7B-Instruct --load-format "npcache"
    environment:
      - NCCL_IGNORE_DISABLED_P2P=1
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://0.0.0.0:8000/v1/models" ]
      interval: 30s
      timeout: 5s
      retries: 20
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

volumes:
  db_data:
